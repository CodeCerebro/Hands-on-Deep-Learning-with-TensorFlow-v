{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import *\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.python.training.adam import AdamOptimizer\n",
    "\n",
    "\n",
    "def ndmatmul(A, B):\n",
    "    get_shape = lambda T, i: T.get_shape()[i] if T.get_shape()[i].value != None else -1\n",
    "    X = tf.reshape(A, shape=[-1, get_shape(A, -1)])\n",
    "    return tf.reshape(tf.matmul(X, B), shape=[get_shape(A, 0), get_shape(A, 1), get_shape(B, 1)])\n",
    "\n",
    "\n",
    "def partition(l, k):\n",
    "    return [l[i:i + k] for i in range(0, len(l), k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self):\n",
    "        tf.set_random_seed(1)\n",
    "        N = 10\n",
    "        L = self.L = 5\n",
    "        self.input = tf.placeholder(dtype=tf.float32, shape=[None, L, 1])\n",
    "        self.desired = tf.placeholder(dtype=tf.float32, shape=[None, L, 1])\n",
    "        self.prev_state = tf.placeholder(dtype=tf.float32, shape=[None, N])\n",
    "        xv = xavier_initializer()\n",
    "        ones = tf.initializers.ones()\n",
    "        self.rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=N, activation=tf.nn.leaky_relu, name=\"\" + str(\n",
    "            random.random()))  # ,kernel_initializer=xv,bias_initializer=ones)\n",
    "        self.wy = tf.Variable(xv(shape=[N, 1]), dtype=tf.float32)\n",
    "        self.by = tf.Variable(ones(shape=[1, 1]), dtype=tf.float32)\n",
    "        self.hidden_acts, self.states = tf.nn.dynamic_rnn(self.rnn_cell, inputs=self.input,\n",
    "                                                          initial_state=self.prev_state)  # None x L x N\n",
    "        self.out = tf.nn.leaky_relu(ndmatmul(self.hidden_acts, self.wy) + self.by)\n",
    "        self.loss = tf.reduce_sum((self.out - self.desired) ** 2)\n",
    "        opt = tf.train.AdamOptimizer()\n",
    "        grads = opt.compute_gradients(self.loss)\n",
    "        clipped = zip(tf.clip_by_global_norm([grad for grad, var in grads if grad != None], 10)[0],\n",
    "                      [var for grad, var in grads if grad != None])\n",
    "        self.train_ = opt.apply_gradients(clipped)\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "    L = self.L\n",
    "    sess = self.sess\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(2 ** L):\n",
    "        x = bin(i)[2:]\n",
    "        x = \"0\" * (L - len(x)) + x\n",
    "        x = list(map(int, x))\n",
    "        y = [x[0]]\n",
    "        for i in x[1:]:\n",
    "            y.append(y[-1] * 2 + i)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    indices = list(range(1, 2 ** L))\n",
    "    random.shuffle(indices)\n",
    "    X = [X[i] for i in indices]\n",
    "    Y = [Y[i] for i in indices]\n",
    "    Xs = partition(X, 50)\n",
    "    Ys = partition(Y, 50)\n",
    "\n",
    "    zero_state_0 = self.rnn_cell.zero_state(batch_size=len(Xs[0]), dtype=tf.float32).eval(session=sess)\n",
    "    zero_state_1 = self.rnn_cell.zero_state(batch_size=len(Xs[-1]), dtype=tf.float32).eval(session=sess)\n",
    "    for epoch in range(1000):\n",
    "        total_loss = 0\n",
    "        for X, Y in zip(Xs, Ys):\n",
    "            zero_state = zero_state_0 if len(X) == len(Xs[0]) else zero_state_1\n",
    "            _, loss, curr_states = sess.run([self.train_, self.loss, self.states],\n",
    "                                            feed_dict={self.input: np.array(X).reshape([-1, L, 1]),\n",
    "                                                       self.desired: np.array(Y).reshape([-1, L, 1]),\n",
    "                                                       self.prev_state: zero_state})\n",
    "            total_loss += loss\n",
    "        if epoch % 100 == 0:\n",
    "            mean_loss = total_loss / 2 ** L\n",
    "            print mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self, seq):\n",
    "    sess = self.sess\n",
    "    L = self.L\n",
    "    outputs = []\n",
    "    prev_state = self.rnn_cell.zero_state(batch_size=1, dtype=tf.float32).eval(session=sess)\n",
    "    while len(seq) >= L:\n",
    "        s = seq[:L]\n",
    "        inp = list(map(int, s))\n",
    "        out, prev_state = self.sess.run([self.out, self.states],\n",
    "                                        feed_dict={self.input: np.array(inp).reshape([1, -1, 1]),\n",
    "                                                   self.prev_state: prev_state})\n",
    "        outputs.extend(out.tolist()[0][:L])\n",
    "        seq = seq[L:]\n",
    "    if len(seq) > 0:\n",
    "        s = seq + \"0\" * (L - len(seq))\n",
    "        inp = list(map(int, s))\n",
    "        out = self.sess.run([self.out],\n",
    "                            feed_dict={self.input: np.array(inp).reshape([1, -1, 1]), self.prev_state: prev_state})[\n",
    "            0]\n",
    "        outputs.extend(out.tolist()[0][:len(seq)])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "rnn = RNN()\n",
    "rnn.train()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383.74114990234375\n",
      "323.4237365722656\n",
      "41.19600296020508\n",
      "4.985841751098633\n",
      "0.9714641571044922\n",
      "0.12593123316764832\n",
      "0.02036137878894806\n",
      "0.0044591547921299934\n",
      "0.0019453082932159305\n",
      "0.0007638398092240095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.9967873096466064],\n",
       " [1.9970052242279053],\n",
       " [4.0078349113464355],\n",
       " [7.992044925689697]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import *\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.python.training.adam import AdamOptimizer\n",
    "\n",
    "\n",
    "def ndmatmul(A, B):\n",
    "    get_shape = lambda T, i: T.get_shape()[i] if T.get_shape()[i].value != None else -1\n",
    "    X = tf.reshape(A, shape=[-1, get_shape(A, -1)])\n",
    "    return tf.reshape(tf.matmul(X, B), shape=[get_shape(A, 0), get_shape(A, 1), get_shape(B, 1)])\n",
    "\n",
    "\n",
    "def partition(l, k):\n",
    "    return [l[i:i + k] for i in range(0, len(l), k)]\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self):\n",
    "        tf.set_random_seed(1)\n",
    "        N = 10\n",
    "        L = self.L = 5\n",
    "        self.input = tf.placeholder(dtype=tf.float32, shape=[None, L, 1])\n",
    "        self.desired = tf.placeholder(dtype=tf.float32, shape=[None, L, 1])\n",
    "        self.prev_state = tf.placeholder(dtype=tf.float32, shape=[None, N])\n",
    "        xv = xavier_initializer()\n",
    "        ones = tf.initializers.ones()\n",
    "        self.rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=N, activation=tf.nn.leaky_relu, name=\"\" + str(\n",
    "            random.random()))  # ,kernel_initializer=xv,bias_initializer=ones)\n",
    "        self.wy = tf.Variable(xv(shape=[N, 1]), dtype=tf.float32)\n",
    "        self.by = tf.Variable(ones(shape=[1, 1]), dtype=tf.float32)\n",
    "        self.hidden_acts, self.states = tf.nn.dynamic_rnn(self.rnn_cell, inputs=self.input,\n",
    "                                                          initial_state=self.prev_state)  # None x L x N\n",
    "        self.out = tf.nn.leaky_relu(ndmatmul(self.hidden_acts, self.wy) + self.by)\n",
    "        self.loss = tf.reduce_sum((self.out - self.desired) ** 2)\n",
    "        opt = tf.train.AdamOptimizer()\n",
    "        grads = opt.compute_gradients(self.loss)\n",
    "        clipped = zip(tf.clip_by_global_norm([grad for grad, var in grads if grad != None], 10)[0],\n",
    "                      [var for grad, var in grads if grad != None])\n",
    "        self.train_ = opt.apply_gradients(clipped)\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train(self):\n",
    "        L = self.L\n",
    "        sess = self.sess\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in range(2 ** L):\n",
    "            x = bin(i)[2:]\n",
    "            x = \"0\" * (L - len(x)) + x\n",
    "            x = list(map(int, x))\n",
    "            y = [x[0]]\n",
    "            for i in x[1:]:\n",
    "                y.append(y[-1] * 2 + i)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "        indices = list(range(1, 2 ** L))\n",
    "        random.shuffle(indices)\n",
    "        X = [X[i] for i in indices]\n",
    "        Y = [Y[i] for i in indices]\n",
    "        Xs = partition(X, 50)\n",
    "        Ys = partition(Y, 50)\n",
    "\n",
    "        zero_state_0 = self.rnn_cell.zero_state(batch_size=len(Xs[0]), dtype=tf.float32).eval(session=sess)\n",
    "        zero_state_1 = self.rnn_cell.zero_state(batch_size=len(Xs[-1]), dtype=tf.float32).eval(session=sess)\n",
    "        for epoch in range(1000):\n",
    "            total_loss = 0\n",
    "            for X, Y in zip(Xs, Ys):\n",
    "                zero_state = zero_state_0 if len(X) == len(Xs[0]) else zero_state_1\n",
    "                _, loss, curr_states = sess.run([self.train_, self.loss, self.states],\n",
    "                                                feed_dict={self.input: np.array(X).reshape([-1, L, 1]),\n",
    "                                                           self.desired: np.array(Y).reshape([-1, L, 1]),\n",
    "                                                           self.prev_state: zero_state})\n",
    "                total_loss += loss\n",
    "            if epoch % 100 == 0:\n",
    "                mean_loss = total_loss / 2 ** L\n",
    "                print(mean_loss)\n",
    "\n",
    "    def run(self, seq):\n",
    "        sess = self.sess\n",
    "        L = self.L\n",
    "        outputs = []\n",
    "        prev_state = self.rnn_cell.zero_state(batch_size=1, dtype=tf.float32).eval(session=sess)\n",
    "        while len(seq) >= L:\n",
    "            s = seq[:L]\n",
    "            inp = list(map(int, s))\n",
    "            out, prev_state = self.sess.run([self.out, self.states],\n",
    "                                            feed_dict={self.input: np.array(inp).reshape([1, -1, 1]),\n",
    "                                                       self.prev_state: prev_state})\n",
    "            outputs.extend(out.tolist()[0][:L])\n",
    "            seq = seq[L:]\n",
    "        if len(seq) > 0:\n",
    "            s = seq + \"0\" * (L - len(seq))\n",
    "            inp = list(map(int, s))\n",
    "            out = self.sess.run([self.out],\n",
    "                                feed_dict={self.input: np.array(inp).reshape([1, -1, 1]), self.prev_state: prev_state})[\n",
    "                0]\n",
    "            outputs.extend(out.tolist()[0][:len(seq)])\n",
    "        return outputs\n",
    "\n",
    "\n",
    "rnn = RNN()\n",
    "rnn.train()\n",
    "rnn.run(\"1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
