{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import *\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.python.training.adam import AdamOptimizer\n",
    "\n",
    "\n",
    "def ndmatmul(A, B):\n",
    "    get_shape = lambda T, i: T.get_shape()[i] if T.get_shape()[i].value != None else -1\n",
    "    X = tf.reshape(A, shape=[-1, get_shape(A, -1)])\n",
    "    return tf.reshape(tf.matmul(X, B), shape=[get_shape(A, 0), get_shape(A, 1), get_shape(B, 1)])\n",
    "\n",
    "\n",
    "def partition(l, k):\n",
    "    return [l[i:i + k] for i in range(0, len(l), k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self):\n",
    "        tf.set_random_seed(1)\n",
    "        N = 10\n",
    "        L = self.L = 5\n",
    "        self.input = tf.placeholder(dtype=tf.float32, shape=[None, L, 1])\n",
    "        self.desired = tf.placeholder(dtype=tf.float32, shape=[None, L, 1])\n",
    "        self.prev_c = tf.placeholder(dtype=tf.float32, shape=[None, N])\n",
    "        self.prev_h = tf.placeholder(dtype=tf.float32,shape=[None,N])\n",
    "        self.prev_state = tf.contrib.rnn.LSTMStateTuple(h=self.prev_h,c=self.prev_c)\n",
    "        xv = xavier_initializer()\n",
    "        ones = tf.initializers.ones()\n",
    "        self.rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=N, activation=tf.nn.leaky_relu)\n",
    "        #, name=\"\" + str(random.random())# ,kernel_initializer=xv,bias_initializer=ones)\n",
    "        self.wy = tf.Variable(xv(shape=[N, 1]), dtype=tf.float32)\n",
    "        self.by = tf.Variable(ones(shape=[1, 1]), dtype=tf.float32)\n",
    "        self.hidden_acts, self.states = tf.nn.dynamic_rnn(self.rnn_cell, inputs=self.input,\n",
    "                                                          initial_state=self.prev_state)  # None x L x N\n",
    "        self.out = tf.nn.leaky_relu(ndmatmul(self.hidden_acts, self.wy) + self.by)\n",
    "        self.loss = tf.reduce_sum((self.out - self.desired) ** 2)\n",
    "        opt = tf.train.AdamOptimizer()\n",
    "        grads = opt.compute_gradients(self.loss)\n",
    "        clipped = zip(tf.clip_by_global_norm([grad for grad, var in grads if grad != None], 10)[0],\n",
    "                      [var for grad, var in grads if grad != None])\n",
    "        self.train_ = opt.apply_gradients(clipped)\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(self):\n",
    "    L = self.L\n",
    "    sess = self.sess\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(2 ** L):\n",
    "        x = bin(i)[2:]\n",
    "        x = \"0\" * (L - len(x)) + x\n",
    "        x = list(map(int, x))\n",
    "        y = [x[0]]\n",
    "        for i in x[1:]:\n",
    "            y.append(y[-1] * 2 + i)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    indices = list(range(1, 2 ** L))\n",
    "    random.shuffle(indices)\n",
    "    X = [X[i] for i in indices]\n",
    "    Y = [Y[i] for i in indices]\n",
    "    Xs = partition(X, 50)\n",
    "    Ys = partition(Y, 50)\n",
    "\n",
    "    zero_state_0 = self.rnn_cell.zero_state(batch_size=len(Xs[0]), dtype=tf.float32).eval(session=sess)\n",
    "    zero_state_1 = self.rnn_cell.zero_state(batch_size=len(Xs[-1]), dtype=tf.float32).eval(session=sess)\n",
    "    for epoch in range(1000):\n",
    "        total_loss = 0\n",
    "        for X, Y in zip(Xs, Ys):\n",
    "            zero_state = zero_state_0 if len(X) == len(Xs[0]) else zero_state_1\n",
    "            _, loss, curr_states = sess.run([self.train_, self.loss, self.states],\n",
    "                                            feed_dict={self.input: np.array(X).reshape([-1, L, 1]),\n",
    "                                                       self.desired: np.array(Y).reshape([-1, L, 1]),\n",
    "                                                       self.prev_h: h,\n",
    "                                                      self.prev_c: c})\n",
    "            total_loss += loss\n",
    "        if epoch % 100 == 0:\n",
    "            mean_loss = total_loss / 2 ** L\n",
    "            print mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(self, seq):\n",
    "    sess = self.sess\n",
    "    L = self.L\n",
    "    outputs = []\n",
    "    prev_state = self.rnn_cell.zero_state(batch_size=1, dtype=tf.float32).eval(session=sess)\n",
    "    while len(seq) >= L:\n",
    "        s = seq[:L]\n",
    "        inp = list(map(int, s))\n",
    "        out, prev_state = self.sess.run([self.out, self.states],\n",
    "                                        feed_dict={self.input: np.array(inp).reshape([1, -1, 1]),\n",
    "                                                   self.prev_h: prev_state.h,\n",
    "                                                       self.prev_c: prev_state.c})\n",
    "        outputs.extend(out.tolist()[0][:L])\n",
    "        seq = seq[L:]\n",
    "    if len(seq) > 0:\n",
    "        s = seq + \"0\" * (L - len(seq))\n",
    "        inp = list(map(int, s))\n",
    "        out = self.sess.run([self.out],\n",
    "                            feed_dict={self.input: np.array(inp).reshape([1, -1, 1]),\n",
    "                                       self.prev_h: prev_state.h,\n",
    "                                           self.prev_c: prev_state.c})[0]\n",
    "        outputs.extend(out.tolist()[0][:len(seq)])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "rnn = RNN()\n",
    "rnn.train()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364.21295166\n",
      "263.604949951\n",
      "11.6644048691\n",
      "2.34909057617\n",
      "0.576039969921\n",
      "0.140497714281\n",
      "0.0431701205671\n",
      "0.0179013479501\n",
      "0.0118889864534\n",
      "0.00901557784528\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LSTMStateTuple' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a55df489cfab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-a55df489cfab>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mprev_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LSTMStateTuple' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import *\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.python.training.adam import AdamOptimizer\n",
    "\n",
    "\n",
    "def ndmatmul(A, B):\n",
    "    get_shape = lambda T, i: T.get_shape()[i] if T.get_shape()[i].value != None else -1\n",
    "    X = tf.reshape(A, shape=[-1, get_shape(A, -1)])\n",
    "    return tf.reshape(tf.matmul(X, B), shape=[get_shape(A, 0), get_shape(A, 1), get_shape(B, 1)])\n",
    "\n",
    "\n",
    "def partition(l, k):\n",
    "    return [l[i:i + k] for i in range(0, len(l), k)]\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self):\n",
    "        tf.set_random_seed(1)\n",
    "        N = 10\n",
    "        L = self.L = 5\n",
    "        self.input = tf.placeholder(dtype=tf.float32, shape=[None, L, 1])\n",
    "        self.desired = tf.placeholder(dtype=tf.float32, shape=[None, L, 1])\n",
    "        self.prev_c = tf.placeholder(dtype=tf.float32, shape=[None, N])\n",
    "        self.prev_h = tf.placeholder(dtype=tf.float32,shape=[None,N])\n",
    "        self.prev_state = tf.contrib.rnn.LSTMStateTuple(h=self.prev_h,c=self.prev_c)\n",
    "        xv = xavier_initializer()\n",
    "        ones = tf.initializers.ones()\n",
    "        self.rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=N, activation=tf.nn.leaky_relu)\n",
    "        #, name=\"\" + str(random.random())# ,kernel_initializer=xv,bias_initializer=ones)\n",
    "        self.wy = tf.Variable(xv(shape=[N, 1]), dtype=tf.float32)\n",
    "        self.by = tf.Variable(ones(shape=[1, 1]), dtype=tf.float32)\n",
    "        self.hidden_acts, self.states = tf.nn.dynamic_rnn(self.rnn_cell, inputs=self.input,\n",
    "                                                          initial_state=self.prev_state)  # None x L x N\n",
    "        self.out = tf.nn.leaky_relu(ndmatmul(self.hidden_acts, self.wy) + self.by)\n",
    "        self.loss = tf.reduce_sum((self.out - self.desired) ** 2)\n",
    "        opt = tf.train.AdamOptimizer()\n",
    "        grads = opt.compute_gradients(self.loss)\n",
    "        clipped = zip(tf.clip_by_global_norm([grad for grad, var in grads if grad != None], 10)[0],\n",
    "                      [var for grad, var in grads if grad != None])\n",
    "        self.train_ = opt.apply_gradients(clipped)\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    def train(self):\n",
    "        L = self.L\n",
    "        sess = self.sess\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in range(2 ** L):\n",
    "            x = bin(i)[2:]\n",
    "            x = \"0\" * (L - len(x)) + x\n",
    "            x = list(map(int, x))\n",
    "            y = [x[0]]\n",
    "            for i in x[1:]:\n",
    "                y.append(y[-1] * 2 + i)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "        indices = list(range(1, 2 ** L))\n",
    "        random.shuffle(indices)\n",
    "        X = [X[i] for i in indices]\n",
    "        Y = [Y[i] for i in indices]\n",
    "        Xs = partition(X, 50)\n",
    "        Ys = partition(Y, 50)\n",
    "\n",
    "        zero_state_0 = self.rnn_cell.zero_state(batch_size=len(Xs[0]), dtype=tf.float32)#.eval(session=sess)\n",
    "        zero_state_1 = self.rnn_cell.zero_state(batch_size=len(Xs[-1]), dtype=tf.float32)#.eval(session=sess)\n",
    "        for epoch in range(1000):\n",
    "            total_loss = 0\n",
    "            for X, Y in zip(Xs, Ys):\n",
    "                zero_state = zero_state_0 if len(X) == len(Xs[0]) else zero_state_1\n",
    "                h = zero_state.h.eval(session=sess)\n",
    "                c = zero_state.c.eval(session=sess)\n",
    "                _, loss, curr_states = sess.run([self.train_, self.loss, self.states],\n",
    "                                                feed_dict={self.input: np.array(X).reshape([-1, L, 1]),\n",
    "                                                           self.desired: np.array(Y).reshape([-1, L, 1]),\n",
    "                                                           self.prev_h: h,\n",
    "                                                          self.prev_c: c})\n",
    "                total_loss += loss\n",
    "            if epoch % 100 == 0:\n",
    "                mean_loss = total_loss / 2 ** L\n",
    "                print mean_loss\n",
    "                \n",
    "    def run(self, seq):\n",
    "        sess = self.sess\n",
    "        L = self.L\n",
    "        outputs = []\n",
    "        prev_state = self.rnn_cell.zero_state(batch_size=1, dtype=tf.float32).eval(session=sess)\n",
    "        while len(seq) >= L:\n",
    "            s = seq[:L]\n",
    "            inp = list(map(int, s))\n",
    "            out, prev_state = self.sess.run([self.out, self.states],\n",
    "                                            feed_dict={self.input: np.array(inp).reshape([1, -1, 1]),\n",
    "                                                       self.prev_h: prev_state.h,\n",
    "                                                           self.prev_c: prev_state.c})\n",
    "            outputs.extend(out.tolist()[0][:L])\n",
    "            seq = seq[L:]\n",
    "        if len(seq) > 0:\n",
    "            s = seq + \"0\" * (L - len(seq))\n",
    "            inp = list(map(int, s))\n",
    "            out = self.sess.run([self.out],\n",
    "                                feed_dict={self.input: np.array(inp).reshape([1, -1, 1]),\n",
    "                                           self.prev_h: prev_state.h,\n",
    "                                               self.prev_c: prev_state.c})[0]\n",
    "            outputs.extend(out.tolist()[0][:len(seq)])\n",
    "        return outputs   \n",
    "        \n",
    "        \n",
    "\n",
    "rnn = RNN()\n",
    "rnn.train()\n",
    "rnn.run(\"1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
